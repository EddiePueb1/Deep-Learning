{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spot quadruped with arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/final/Deep-Learning/DL/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/ubuntu/final/Deep-Learning/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "\n",
    "env = gymnasium.make(\n",
    "    \"Ant-v5\",\n",
    "    xml_file=\"./robots/boston_dynamics_spot/scene_arm.xml\",\n",
    "    forward_reward_weight=1,  # kept the same as the 'Ant' environment\n",
    "    ctrl_cost_weight=0.05,  # changed because of the stronger motors of `Go1`\n",
    "    contact_cost_weight=5e-4,  # kept the same as the 'Ant' environment\n",
    "    healthy_reward=1,  # kept the same as the 'Ant' environment\n",
    "    main_body=1,  # represents the \"trunk\" of the `Go1` robot\n",
    "    healthy_z_range=(0.195, 0.95),\n",
    "    include_cfrc_ext_in_observation=True,\n",
    "    exclude_current_positions_from_observation=False,\n",
    "    reset_noise_scale=0.1,\n",
    "    frame_skip=25,\n",
    "    max_episode_steps=1000,\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "\n",
    "env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda episode_id: True)\n",
    "env = RecordEpisodeStatistics(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"MUJOCO_GL\"] = \"egl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "import mujoco\n",
    "\n",
    "class LocoManipulationEnv(gym.Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode=None, reward_fn=None):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.reward_fn = reward_fn if reward_fn is not None else self.default_reward\n",
    "\n",
    "        # Load MuJoCo model\n",
    "        self.model = mujoco.MjModel.from_xml_path(\"./robots/boston_dynamics_spot/scene_arm.xml\")\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "\n",
    "        # Renderer for rgb_array mode\n",
    "        if render_mode == \"rgb_array\":\n",
    "            self.frame_width = 640\n",
    "            self.frame_height = 480\n",
    "            self._renderer = mujoco.Renderer(self.model, width=self.frame_width, height=self.frame_height)\n",
    "\n",
    "        self.nq = self.model.nq\n",
    "        self.nv = self.model.nv\n",
    "        self.nu = self.model.nu\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(self.nq + self.nv,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=self.model.actuator_ctrlrange[:, 0],\n",
    "            high=self.model.actuator_ctrlrange[:, 1],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def default_reward(self, obs, action):\n",
    "        return -np.linalg.norm(obs)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "\n",
    "        obs = np.concatenate([self.data.qpos, self.data.qvel]).astype(np.float32)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.data.ctrl[:] = action\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "\n",
    "        obs = np.concatenate([self.data.qpos, self.data.qvel]).astype(np.float32)\n",
    "        reward = self.reward_fn(obs, action)\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            self._renderer.update_scene(self.data)\n",
    "            return self._renderer.render()\n",
    "        elif self.render_mode == \"human\":\n",
    "            if not hasattr(self, \"viewer\"):\n",
    "                import mujoco.viewer\n",
    "                self.viewer = mujoco.viewer.launch_passive(self.model, self.data)\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        if hasattr(self, \"_renderer\"):\n",
    "            self._renderer.close()\n",
    "        if hasattr(self, \"viewer\"):\n",
    "            self.viewer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reward_code = \"\"\"\n",
    "def reward_fn(obs, action):\n",
    "    # Example: move end-effector to a goal with smooth control\n",
    "    pos = obs[:3]  # Replace with correct index for end-effector\n",
    "    goal = np.array([0.3, 0.2, 0.1])\n",
    "    dist = np.linalg.norm(pos - goal)\n",
    "    energy = np.linalg.norm(action)\n",
    "    return -dist - 0.1 * energy\n",
    "\"\"\"\n",
    "\n",
    "local_vars = {}\n",
    "exec(reward_code, {\"np\": np}, local_vars)\n",
    "generated_reward_fn = local_vars[\"reward_fn\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mujoco' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m env = \u001b[43mLocoManipulationEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrgb_array\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerated_reward_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m env = RecordVideo(env, video_folder=\u001b[33m\"\u001b[39m\u001b[33m./videos\u001b[39m\u001b[33m\"\u001b[39m, episode_trigger=\u001b[38;5;28;01mlambda\u001b[39;00m episode_id: \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m env = RecordEpisodeStatistics(env)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mLocoManipulationEnv.__init__\u001b[39m\u001b[34m(self, render_mode, reward_fn)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.reward_fn = reward_fn \u001b[38;5;28;01mif\u001b[39;00m reward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_reward\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load MuJoCo model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mmujoco\u001b[49m.MjModel.from_xml_path(\u001b[33m\"\u001b[39m\u001b[33m./robots/boston_dynamics_spot/scene_arm.xml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.data = mujoco.MjData(\u001b[38;5;28mself\u001b[39m.model)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Renderer for rgb_array mode\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'mujoco' is not defined"
     ]
    }
   ],
   "source": [
    "env = LocoManipulationEnv(render_mode=\"rgb_array\", reward_fn=generated_reward_fn)\n",
    "env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda episode_id: True)\n",
    "env = RecordEpisodeStatistics(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(4):\n",
    "    print(f\"Starting episode {ep + 1}\")\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    reward_total = 0\n",
    "    step_count = 0\n",
    "\n",
    "    while not done and step_count < 200:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        env.render()\n",
    "        reward_total += reward\n",
    "        done = terminated or truncated\n",
    "        step_count += 1\n",
    "\n",
    "    print(f\"Episode {ep + 1} ended with reward {reward_total:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_shape, action_shape, hidden_dim, std=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.std = std\n",
    "        self.policy = nn.Sequential(nn.Linear(obs_shape[0], hidden_dim),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Linear(hidden_dim, action_shape[0]))\n",
    "\n",
    "        self.apply(utils.weight_init)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mu = self.policy(obs)\n",
    "        mu = torch.tanh(mu)\n",
    "        std = torch.ones_like(mu) * self.std\n",
    "\n",
    "        dist = utils.TruncatedNormal(mu, std)\n",
    "        return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([[1,2], [3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
